# Apple CSAM Software Controversy

The article that I chose is about the controversy regarding Apple's new (but not yet released) CSAM detection
software: https://techcrunch.com/2021/09/03/apple-csam-detection-delayed/ 

## My Thoughts

CSAM Software is software that analyzes images to determine if they contain CSAM (Child Sex Abuse Material), 
and Apple is planning to roll their own CSAM Software, "NeuralHash", that will work within images on user devices.

I think this situation is very interesting because there are two understandable sides to the argument. The first is that users don't want Apple accessing the images on their devices, as it is a privacy concern. Apple has stated that the NeuralHash technology operates "without having to possess the image or knowing the contents of the image". Many people, including the American Civil Liberties Union, have urged Apple to not release their NeuralHash software. But on the other hand, Apple and others (including the National Center for Missing and Exploited Children, whom Apple worked with when developing the software) want to catch child predators and end child sex abuse.

I think that this software would be helpful, but I also understand concerns that other entities (like governments) using this software for bad purposes - which is entirely possible. However, if NeuralHash is actually efficient and demonstrates ability to catch child predators, I think that this conversation over privacy needs to be re-visited. If the software truly doesn't know the actual content of the images, like Apple says it doesn't, then I think NeuralHash would be a great tool. I will continue to follow this story as it develops, and as Apple decides whether or not it will release NeuralHash or not.